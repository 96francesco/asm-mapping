{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Externals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import rasterio\n",
    "\n",
    "from pathlib import Path\n",
    "from pytorch_lightning import seed_everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Internal modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from asm_mapping.data.planetscope_dataset import PlanetScopeDataset\n",
    "from asm_mapping.data.sentinel1_dataset import Sentinel1Dataset\n",
    "from asm_mapping.data.fusion_dataset import FusionDataset\n",
    "from asm_mapping.data.dataset_mode import DatasetMode\n",
    "from asm_mapping.models.lit_model_standalone import LitModelStandalone\n",
    "from asm_mapping.models.lit_model_lf import LitModelLateFusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seeds\n",
    "RANDOM = 79\n",
    "seed_everything(RANDOM, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_ID = 3\n",
    "\n",
    "# Set device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(f\"cuda:{GPU_ID}\")\n",
    "    print(f\"Using GPU #{GPU_ID}: {torch.cuda.get_device_name(GPU_ID)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA not available, using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"/mnt/guanabana/raid/home/pasan001/asm-mapping\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PS_MODEL_PATH = f\"{BASE_DIR}/checkpoints/split_0/ps_standalone_split_0_epoch=31_val_f1_score=0.000.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PS_DATA_PATH = f\"{BASE_DIR}/data/ps_split/split_0/testing_set\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset parameters\n",
    "PAD = False\n",
    "TRANSFORMS = None\n",
    "STANDALONE_MODE = DatasetMode.STANDALONE\n",
    "FUSION_MODE = DatasetMode.FUSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_standalone_model(checkpoint_path, in_channels=6):\n",
    "      torch.cuda.set_device(GPU_ID)\n",
    "      checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "      model = LitModelStandalone.load_from_checkpoint(checkpoint_path, \n",
    "                                                      in_channels=in_channels,\n",
    "                                                      map_location=device)\n",
    "      model.eval()\n",
    "      return model\n",
    "\n",
    "def load_fusion_model(checkpoint_path):\n",
    "      torch.cuda.set_device(GPU_ID)\n",
    "      checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "      model = LitModelLateFusion.load_from_checkpoint(checkpoint_path,\n",
    "                                                      map_location=device)\n",
    "      model.eval()\n",
    "      return model\n",
    "\n",
    "def predict_standalone(model, img_tensor):\n",
    "      with torch.no_grad():\n",
    "            img_batch = img_tensor.unsqueeze(0)\n",
    "            # ensure tensor is on the same device as model\n",
    "            img_batch = img_batch.to(device)\n",
    "            logits = model(img_batch)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            pred = (probs > model.threshold).float().squeeze().cpu().numpy()\n",
    "      return pred\n",
    "\n",
    "def predict_fusion(model, planet_tensor, s1_tensor):\n",
    "      with torch.no_grad():\n",
    "            planet_batch = planet_tensor.unsqueeze(0)\n",
    "            s1_batch = s1_tensor.unsqueeze(0)\n",
    "            # ensure tensors are on the same device as model\n",
    "            planet_batch = planet_batch.to(device)\n",
    "            s1_batch = s1_batch.to(device)\n",
    "            logits = model(planet_batch, s1_batch)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            pred = (probs > model.threshold).float().squeeze().cpu().numpy()\n",
    "      return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_rgb(dataset, idx):\n",
    "      img_path = dataset.dataset[idx][0]\n",
    "      \n",
    "      with rasterio.open(img_path, 'r') as src:\n",
    "            img = src.read().astype(np.float32)\n",
    "      \n",
    "      rgb = np.zeros((img.shape[1], img.shape[2], 3))\n",
    "      for i, band_idx in enumerate([2, 1, 0]):\n",
    "            band = img[band_idx]\n",
    "            band_min, band_max = band.min(), band.max()\n",
    "            if band_max > band_min: \n",
    "                  rgb[:,:,i] = np.clip((band - band_min) / (band_max - band_min), 0, 1)\n",
    "            else:\n",
    "                  rgb[:,:,i] = 0\n",
    "      \n",
    "      return rgb\n",
    "\n",
    "def plot_ps_predictions(model, dataset, indices=None, num_examples=3):\n",
    "      if indices is None:\n",
    "            indices = random.sample(range(len(dataset)), num_examples)\n",
    "      \n",
    "      fig, axs = plt.subplots(num_examples, 3, figsize=(15, num_examples * 4))\n",
    "      \n",
    "      for i, idx in enumerate(indices):\n",
    "            img_tensor, gt_tensor = dataset[idx]\n",
    "            pred = predict_standalone(model, img_tensor)\n",
    "            \n",
    "            # get RGB for visualization\n",
    "            rgb = get_raw_rgb(dataset, idx)\n",
    "            \n",
    "            # make plots\n",
    "            axs[i, 0].imshow(rgb)\n",
    "            axs[i, 0].set_title(f\"PlanetScope RGB - Example {i+1}\")\n",
    "            axs[i, 0].axis('off')\n",
    "            \n",
    "            axs[i, 1].imshow(pred, cmap='gray')\n",
    "            axs[i, 1].set_title(f\"Model Prediction\")\n",
    "            axs[i, 1].axis('off')\n",
    "            \n",
    "            axs[i, 2].imshow(gt_tensor.numpy(), cmap='gray')\n",
    "            axs[i, 2].set_title(f\"Ground Truth\")\n",
    "            axs[i, 2].axis('off')\n",
    "      \n",
    "      plt.tight_layout()\n",
    "      plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_dataset = PlanetScopeDataset(\n",
    "    data_dir=PS_DATA_PATH,\n",
    "    mode=STANDALONE_MODE,\n",
    "    pad=PAD,\n",
    "    transforms=TRANSFORMS,\n",
    "    split=\"split_0\" \n",
    ")\n",
    "\n",
    "try:\n",
    "    ps_model = load_standalone_model(PS_MODEL_PATH, in_channels=6)\n",
    "    print(\"PlanetScope model loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading PlanetScope model: {e}\")\n",
    "    ps_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ps_model is not None:\n",
    "    print(\"\\n## PlanetScope Standalone model predictions\")\n",
    "    fixed_indices = [87, 1, 111]\n",
    "    plot_ps_predictions(ps_model, ps_dataset, indices=fixed_indices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
